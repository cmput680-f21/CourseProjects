# CMPUT 680 Projects

## Profitability Analysis of Data Reorganization to Enable Compiler Assisted Active Lane Consolidation

Wyatt Praharenka designed a technique, Active Lane Consolidation (ACL), that increases the vector unit utilization in vectorized loops with divergent control flow.
In one of ACL's incarnations, multiple iterations of a vectorized loop are inspected to compute ahead-of-time the conditions associated with each vector lane.
ACL then consolidate/merges actives lanes into a vector register until it obtains a uniform vector -- all its lanes are active.
The merged uniform vectors are used for the loop computations and thus fully utilize the CPU's vector unit.
The assemble of uniform vectors requires data to be accessed in a different order than in the original vectorized loop.
Changes in data access order may lead to worse performance due memory hierarchy effects -- e.g increase in cache misses.

Packing is a widespread technique for improving cache utilization by reorganizing data in the same order in which it will be accessed.
The data is reorganized in parts and each portion of the data is copied into temporary buffer in the same order of the memory access in loops.
Packing is heavily used to implement high-performance linear algebra libraries and to increase cache temporal and spatial locality when operating larger matrices.
However, packing adds overhead and such overhead needs to be amortized by the performance improvements that are possible when data is reused multiple times.
Packing could be coupled with ACL to improve cache locality when vectors are merged as data is delivered in the same order of the memory accesses.
The main limitation that stops packing from being used with ACL is that the conditions used to consolidate vectors might only be known at run time, and packing is traditionally implemented with compile-time knowledge of the data access order.

The goal of this project is to perform a limit study to discover if the aforementioned memory hierarchy effects with ACL are indeed a performance problem.
The assessment can suppose the existence of an oracle that knows the exact access order at compile time.
Then data can be packed in the order known by the oracle.
Experiments contrasting the performance of ACL with and without packing will reveal: (1) if memory hierarchy effects limit the application of ACL and (2) if packing can be used to increase the cases in which ACL can be applied by decreasing the memory hierarchy effects.

References:

  * [Wyatt's M.Sc. thesis](PraharenkaMSc.pdf).

## Understanding Why Modern Auto-Vectorizing Compilers Fail to Vectorize Loops

Vectorization is a widely used loop optimization technique that increases application throughput.
Manual vectorization of loops is a complex and error-prone task and thus modern compilers automate it through a set of loop transformation passes.
Vectorization passes rely on legality and profitability checks to decide if loops are vectorized.
Such checks depend on data-flow analysis passes to collect information about the target loop.
Legality checks depend on information such as memory dependencies -- e.g. from array access  -- and the loop's control-flow structure -- e.g. nesting level.
Even more information might be collected to decide on the profitability of vectorizing a loop.
A loop is deemed profitable to vectorize based, for instance, on the data-types of variables used, memory access alignment, induction variable properties (e.g. monotonicity and step), and knowledge on the termination condition or bound of a loop.
The frontend of modern compilers do encode some language-level information into the compiler's optimization representation -- Intermediate Representation (IR).
However, the amount of information available to an auto-vectorizing compiler can vary depending on whether programmers use non-standard attributes, compilation flags, code annotations.

Recent studies show how dependent sensitive different auto-vectorizing compilers are to information withdraw.
Most, if not all, compilers are less capable of vectorizing loops when one or more key loop/program properties are missing.
Missing a single property might prevent vectorizing from happening at all.
On the other hand, and even if two compilers have the same amount of information, a loop might be vectorized by one compiler but not the other.
In this direction, the goal of this project is to assess the loop vectorization passes in LLVM.
More specifically, the project should determine the most common reasons that stop LLVM's vectorization passes from vectorizing loops.
The assessment must contrast the effectiveness of LLVM's vectorizers against other auto-vectorizing compilers (e.g. GCC, ICC, and XLC).

References:

  * [Evaluating Auto-Vectorizing Compilers through Objective Withdrawal of Useful Information](https://dl.acm.org/doi/abs/10.1145/3356842)

## Data-flow Analysis to Determine the Legality of Python Code Modernization Transformations

The amount of available main memory is one of the limiting factors when processing the ever-increasing amount of data that is generated on a daily basis.
Python is one of the current modern languages that shows growing popularity amount data-scientists for its user-friendliness to both novice programmers and non-computer savvy users.
Although prototyping and writing utility scripts in Python is simple, non-seasoned Python programmers might face hard limitations as the size of datasets processed by such small Python programs grows.
For example, opening a file and extracting raw data from it -- e.g. when summarizing results -- is a common idiom in many Python programs.
The simplest way to implement such idiom in Python is to (1) call builtin or module function to open, (2) read the file, and (3) use a loop to iterate over the data read from the file.
While (1) is usually a constant time operation, (2) depends on how large the file being read is.
In addition, in (2) the entire file is read, usually, into a Python list stored in main memory.
If the file is larger than the available main memory the Python program will never be able to process the whole file.
Experienced Python programmers, or those tuned with the latest Python features, know a more efficient way to implement the same idiom using the concept of generators.
A generator in python is an object that allows on-demand data consumption.
For example, instead of reading the whole file into a list and then applying a summarization function that consumes each list element, a generator that reads each element and passes it to such a function can be used.
With a generator, each data element is read from the file, as needed by the consuming function or Python expression, without creating a large in-memory list that would be dead after the consuming expression.
A generator can be any Python expression -- with the yield expression --  and thus can also be applied at the granularity of a function --- with the yield statement.
The goal of this project is to identify opportunities to modernize Python code by creating a transparent and automated way to use novel Python features.
The project should create a data-flow analysis that determines the legality of applying modernizing transformations.

References:

  * [Quantifying the Transition from Python 2 to 3: An Empirical Study of Python Applications](https://ieeexplore.ieee.org/abstract/document/8170118)

## Detecting Code Idioms in LLVM's Intermediate Representation

Code idioms are frequent constructs in programs that express a computation, are easily recognized (by humans), and are simple to compose.
Some idioms become so common that they are incorporated into languages as alternative ways to write frequently used code.
In the C language, incrementing a variable or accumulating into a variable are idioms found potentially in any program.
As a result, the C language designers added shorthand ways to express such idioms -- `i++` or `++i` instead of `i = i + 1` and `c += a*b` instead of `c = c + a*b`.
However, there are more interesting idioms that would be profitable for the compiler to be able to detect in order to generate more efficient code.

One class of idioms that are of high interest are those in the linear-algebra domain.
The General Matrix-Matrix Multiplication (GEMM) is a good example of such idioms.
GEMM operations are widely used in areas such as machine learning, image processing, and scientific computing.
Writing a correct implementation of GEMM is not a hard task.
Implementing one that executes close to peak performance, however, is known to be significantly harder.
This difficulty observation motivated the creation of libraries that provide high-performance implementations of basic linear algebra routines (BLAS).
Nevertheless, many programs have their own, less performant, implementations of BLAS routines.
Changing their code to call BLAS libraries requires manual changes to the code that might introduce bugs and require further testing.

KernelFaRer, an automatic idiom recognizer and replacement algorithm, was designed to capture such idioms and transparently replace them by high-performance library calls.
KernelFaRer is fully implemented as an LLVM IR pass and detects idioms in programs writing in any language that has a LLVM IR frontend.
KerneFaRer extends LLVM's PatternMatch to facilitate the construction of novel pattern descriptions to detect other idioms.

The following two projects are based on KernelFaRer

### Implement Pattern Descriptions to Detect New Idiom(s)

The goal of this project is to write a pattern description to detect a new pattern.
The project can either build on top of KernelFaRer and detect other linear algebra patterns or use KernelFaRer as an inspiration to write patterns descriptions for non-numerical patterns (e.g. accessing/updating a data-structure, sorting a list).
Both correctness and completeness should be taken into consideration for the project.
Reasonable test should indicate that the pattern description allows matches instances of the target idiom, i.e. there are no false positives.
In addition, the tests should also indicate that a significant number of variants of an idiom are detectable by the pattern description.

### Study Transformation Passes to Match Hidden Instances of Idioms

There are cases where computations that are not direct instances of a given idiom could be transformed to use an idiom's implementation as a building block.
Examples are Doolittle's LU Decomposition and Multiresolution Analysis Kernel [Polybench].
Neither of those use the GEMM idiom as a building block but could use that idiom after some code transformations.
In this direction, the goal of project is to investigate which compiler transformation could enable KernelFaRer to replace parts of a computation with calls to high-performance libraries such as BLAS.

References:

  * [KernelFaRer: Replacing Native-Code Idioms with High-Performance Library Calls](https://dl.acm.org/doi/abs/10.1145/3459010)
  * [OpenBLAS](https://github.com/xianyi/OpenBLAS)
  * [Polybench](https://web.cse.ohio-state.edu/~pouchet.2/software/polybench/)

## Assessing Potentially Missed Optimization Opportunities in LLVM

Designing compiler transformation passes is not an easy task.
Besides guaranteeing that transformations are correct -- i.e. that they preserve the observable behavior of the program --, designers need to try not to miss opportunities to apply their code transformation by using as much as possible the available analysis results.
In traditional compiler development, correctness is the main focus and is part of the development and testing framework.
This correctness-focused validation usually fails to identify missed optimization opportunities.
Thus, many easy-to-correct performance regressions are not detected by current compiler development practices.
To address the need to also test the coverage of modern compiler transformations, NULLSTONE was developed.
NULLSTONE is a automated compiler performance analysis tool that uses a QA approach for testing coverage and isolation to measure a compiler's optimizer.

The project's main goal is to use NULLSTONE as a tool compare LLVM's abilities to optimize code against GCC's optimization passes.
The project aims at identifying opportunities that might be missed by LLVM and point to the root cause that stops LLVM, not GCC, to optimize the code, or vice-versa.
The results must highlight the main transformation passes that miss opportunities and the main reasons that prevent LLVM to apply code transformations.

References:

  * [NULLSTONE](http://www.nullstone.com/index.htm)

## Cost-Benefit Analysis of Applying Custom Optimization Pipelines with LLVM

Optimization pipelines are complex to design because different program codes might get better optimized by applying different optimizations.
In addition, even if the same set of optimizations benefit different codes, it might be that applying transformations in different orders might produce better results than using a single order on different codes.
Compiler engineers compromise by deciding on a static pipeline to optimize well the common case -- guided by a selected number of benchmarks -- instead of the impossible design of having infinitely many pipelines, one for each possible program.
Therefore, there are optimizations that might be executed on key program functions that are not actually transforming or optimizing the code.
In addition, most transformations require analysis information to be collected in order to access feasibility and benefit to apply the transformation.
Collecting information and perform code analysis can be expensive.
With the compromised decision of having static pipelines and the potential cost without benefit of running some transformations, this project aims to understand the cost-benefit of applying a custom pipeline in frequently executed code (hot code).
The project must investigate the benefits in compilation time of running a smaller number of optimizations and the performance implications of reducing the set of transformations applied on hot code.
The results should indicate whether designing a custom optimization pipeline provides compilation time reductions without significantly sacrificing overall applications performance.

References:

  * [Applying Support Vector Machines to Discover Just-in-Time Method-Specific Compilation Strategies](https://webdocs.cs.ualberta.ca/~amaral/thesis/RicardoNSanchezMSc.pdf)

## An Exploration of LLVM's libFuzzer on Identifying Bugs by Input Fuzzing

Constructing compilers is a very complex task.
Testing the correctness of compilers is of comparable or greater difficulty.
Fuzzing, a technique for generating random programs, showed to be very effective in discovering bugs in many compilers.
LLVM has a maturing project that aims at constructing fuzzers for many if not all of its components.
The project's main goal is to explorer the existing capabilities and limitations in LLVM's libFuzzer to identify bugs.
The exploration should focus on either coverage or correctness bugs and contrast LLVM against GCC with bugs identified during the project of recently reported bugs that are yet not fixed in LLVM.


References:

  * [Finding and Understanding Bugs in C Compilers](https://dl.acm.org/doi/abs/10.1145/1993498.1993532)
  * [Test-Case Reduction for C Compiler Bugs](https://dl.acm.org/doi/10.1145/2345156.2254104)
  * [libFuzzer](https://llvm.org/docs/LibFuzzer.html)
  * [libFuzzer Tutorial](https://github.com/google/fuzzing/blob/master/tutorial/libFuzzerTutorial.md)
