# CMPUT 680 Projects

## Profitability Analysis of Data Reorganization to Enable Compiler Assisted Active Lane Consolidation

Wyatt Praharenka designed a technique, Active Lane Consolidation (ACL), that increases the vector unit utilization in vectorized loops with divergent control flow.
In one of ACL's incarnations, multiple iterations of a vectorized loop are inspected to compute ahead-of-time the conditions associated with each vector lane.
ACL then consolidate/merges actives lanes into a vector register until it obtains a uniform vector -- all its lanes are active.
The merged uniform vectors are used for the loop computations and thus fully utilize the CPU's vector unit.
The assemble of uniform vectors requires data to be accessed in a different order than in the original vectorized loop.
Changes in data access order may lead to worse performance due memory hierarchy effects -- e.g increase in cache misses.

Packing is a widespread technique for improving cache utilization by reorganizing data in the same order in which it will be accessed.
The data is reorganized in parts and each portion of the data is copied into temporary buffer in the same order of the memory access in loops.
Packing is heavily used to implement high-performance linear algebra libraries and to increase cache temporal and spatial locality when operating larger matrices.
However, packing adds overhead and such overhead needs to be amortized by the performance improvements that are possible when data is reused multiple times.
Packing could be coupled with ACL to improve cache locality when vectors are merged as data is delivered in the same order of the memory accesses.
The main limitation that stops packing from being used with ACL is that the conditions used to consolidate vectors might only be known at run time, and packing is traditionally implemented with compile-time knowledge of the data access order.

The goal of this project is to perform a limit study to discover if the aforementioned memory hierarchy effects with ACL are indeed a performance problem.
The assessment can suppose the existence of an oracle that knows the exact access order at compile time.
Then data can be packed in the order known by the oracle.
Experiments contrasting the performance of ACL with and without packing will reveal: (1) if memory hierarchy effects limit the application of ACL and (2) if packing can be used to increase the cases in which ACL can be applied by decreasing the memory hierarchy effects.

References:

  * [Wyatt's M.Sc. thesis](Add-link-here).

## Understanding Why Modern Auto-Vectorizing Compilers Fail to Vectorize Loops

Vectorization is a widely used loop optimization technique that increases application throughput.
Manual vectorization of loops is a complex and error-prone task and thus modern compilers automate it through a set of loop transformation passes.
Vectorization passes rely on legality and profitability checks to decide if loops are vectorized.
Such checks depend on data-flow analysis passes to collect information about the target loop.
Legality checks depend on information such as memory dependencies -- e.g. from array access  -- and the loop's control-flow structure -- e.g. nesting level.
Even more information might be collected to decide on the profitability of vectorizing a loop.
A loop is deemed profitable to vectorize based, for instance, on the data-types of variables used, memory access alignment, induction variable properties (e.g. monotonicity and step), and knowledge on the termination condition or bound of a loop.
The frontend of modern compilers do encode some language-level information into the compiler's optimization representation -- Intermediate Representation (IR).
However, the amount of information available to an auto-vectorizing compiler can vary depending on whether programmers use non-standard attributes, compilation flags, code annotations.

Recent studies show how dependent sensitive different auto-vectorizing compilers are to information withdraw.
Most, if not all, compilers are less capable of vectorizing loops when one or more key loop/program properties are missing.
Missing a single property might prevent vectorizing from happening at all.
On the other hand, and even if two compilers have the same amount of information, a loop might be vectorized by one compiler but not the other.
In this direction, the goal of this project is to assess the loop vectorization passes in LLVM.
More specifically, the project should determine the most common reasons that stop LLVM's vectorization passes from vectorizing loops.
The assessment must contrast the effectiveness of LLVM's vectorizers against other auto-vectorizing compilers (e.g. GCC, ICC, and XLC).

References:

  * [Evaluating Auto-Vectorizing Compilers through Objective Withdrawal of Useful Information](https://dl.acm.org/doi/abs/10.1145/3356842)

## Data-flow Analysis to Determine the Legality of Python Code Modernization Transformations

The amount of available main memory is one of the limiting factors when process the ever-increasing amount of data that is generated on a daily basis.
Python is one of the current modern languages that shows growing popularity amount data-scientists for its user-friendliness to both novice programmers and non-computer savvy audiences.
Although prototyping and writing utility scripts in Python is simple, non-seasoned Python programmers might face hard limitations as the size of datasets processed by such small Python programs grows.
For example, opening a file and extract raw data from it -- e.g. when summarizing results -- is a commonly found idiom in many Python programs.
The most simple way to implement such idiom in Python is to (1) call builtin or module function to open, (2) read the file, and (3) use a loop to iterate over the data read from the file.
While (1) is usually a constant time operation, (2) depends on how large the file being read is.
In addition, (2) read the entire file is read, usually, into a Python list stored in main memory.
If the file is larger than the available main memory the Python program will never be able to process the whole file.
Experienced Python programmers, or those tuned with the latest Python features, known a more efficient way to implement the same idiom with the concept of generators.
A generator in python is an object that allows one to consume data in a on-demand manner.
For example, instead of reading the whole file into a list and then applying a summarization function that consumes each list element, a generator that reads each element and passes it to the said function can be used.
With a generator, each data element is read from the file as needed by the consuming function or Python expression without creating a large in-memory list that would be dead after the consuming expression.
A generator can be any Python expression -- with the yield expression --  and thus can also be applied at the granularity of a function --- with the yield statement.
In this direction, the goal of this project is to identify opportunities to modernize Python code with a transparent and automated way to use novel Python features.
The project should create a data-flow analysis that determines the legality of applying modernizing transformations.

References:

  * [Quantifying the Transition from Python 2 to 3: An Empirical Study of Python Applications](https://ieeexplore.ieee.org/abstract/document/8170118)


## Detecting Code Idioms in LLVM's Intermediate Representation

Code idioms are frequently found constructs in programs that express a computation, are easily recognized (by humans), and are simple to compose.
Some idioms become so common that they are incorporated into languages as alternative ways to write frequently used code.
In the C language, incrementing a variable or accumulating into a variable are idioms found potentially in any program.
As a result, the C language designers added shorthand ways to express such idioms -- `i++` or `++i` instead of `i = i + 1` and `c += a*b` instead of `c = c + a*b`.
However, there are more interesting idioms that would be profitable for the compiler to be able to detect so more efficient code for them could be generated.

One class of idioms that are of high interest are those in the linear algebra domain.
The General Matrix-Matrix Multiplication (GEMM) is a good example of such idioms.
GEMM operations are widely used in areas like machine learning, image processing, and scientific computing.
Writing a correct implementation of GEMM is not a hard task.
Implementing one that executes close to peak performance, however, is known to be significantly harder.
This difficulty observation motivated the creation of libraries which provide high-performance implementations of basic linear algebra routines (BLAS).
Nevertheless, many programs have their own and less performant implementations of BLAS routines.
Changing their code to call BLAS libraries requires manual changes to the code that might introduce bugs and requires further testing.

KernelFaRer, an automatic idiom recognizer and replacement algorithm, was designed to capture such idioms and transparently replace them by high-performance library calls.
KernelFaRer is fully implemented as a LLVM IR pass and detects idioms in programs writing in any language that has a LLVM IR frontend.
KerneFaRer extends LLVM's PatternMatch to facilitate the construction of novel pattern descriptions to detect other idioms.

### Implement Pattern Descriptions to Detect New Idiom(s)

The goal of this project is to write a pattern description to detect a new pattern.
The project can either build on top of KernelFaRer and detect other linear algebra patterns or use KernelFaRer as an inspiration to write patterns descriptions for non-numerical patterns (e.g. accessing/updating a data-structure, sorting a list).
Both correctness and completeness should be taken into consideration for the project.
Reasonable test should indicate the the pattern description allows only instances of the target idiom to be detected, i.e. no false positives.
In addition, the tests should also indicate that a significant number of variants of an idiom are detectable by the pattern description.

### Study Transformation Passes to Match Hidden Instances of Idioms

There are cases where computations that are not direct instances of a given idiom could be transformed to use a idioms implementation as a building block.
Examples are Doolittle's LU Decomposition and Multiresolution Analysis Kernel [Polybench].
Both do not use the GEMM idiom as a building block but could after some code transformations.
In this direction, the goal of project is to investigate which compiler transformation could enable KernelFaRer to replace parts of a computation with calls to high-performance libraries like BLAS.

References:

  * [KernelFaRer: Replacing Native-Code Idioms with High-Performance Library Calls](https://dl.acm.org/doi/abs/10.1145/3459010)
  * [OpenBLAS](https://github.com/xianyi/OpenBLAS)
  * [Polybench](https://web.cse.ohio-state.edu/~pouchet.2/software/polybench/)
